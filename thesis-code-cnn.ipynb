{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport scipy.io\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomMatDataset(Dataset):\n    def __init__(self, ch_dir, f_dir):\n        self.ch_files = [os.path.join(ch_dir, file) for file in os.listdir(ch_dir) if file.endswith('.mat')]\n        self.f_files = [os.path.join(f_dir, file) for file in os.listdir(f_dir) if file.endswith('.mat')]\n\n    def __len__(self):\n        return len(self.ch_files)\n\n    def __getitem__(self, idx):\n        ch_data = scipy.io.loadmat(self.ch_files[idx])['H']\n        ch_data = np.abs(ch_data)\n        f_data = scipy.io.loadmat(self.f_files[idx])['Fopt_final']\n        f_data = np.abs(f_data)\n        ch_data = np.transpose(ch_data, (2, 0, 1))*1e6  # Change to [5, 36, 144]\n        f_data = np.transpose(f_data, (2, 0, 1)) # Change to [5, 144, 2]\n        return torch.tensor(ch_data, dtype=torch.float32), torch.tensor(f_data, dtype=torch.float32)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define directories\nch_dir = '/kaggle/input/dataset-hfw/Combined Dataset/Input'\nf_dir = '/kaggle/input/dataset-hfw/Combined Dataset/Output1'\n\n# Create dataset\ndataset = CustomMatDataset(ch_dir, f_dir)\n\n# Split dataset into train and test\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.relu = nn.LeakyReLU()\n        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(in_channels)\n    \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out += residual\n        out = self.relu(out)\n        return out\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n        \n        # Define the encoder (contracting path)\n        self.enc_conv0 = self.double_conv(5, 16)\n        self.res0 = ResidualBlock(16)\n        self.enc_conv1 = self.double_conv(16, 32)\n        self.res1 = ResidualBlock(32)\n        self.enc_conv2 = self.double_conv(32, 64)\n        self.res2 = ResidualBlock(64)\n        self.enc_conv3 = self.double_conv(64, 128)\n        self.res3 = ResidualBlock(128)\n        self.pool  = nn.MaxPool2d(2)\n        self.lin = nn.Linear(9216, 1440)\n        \n    def double_conv(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.LeakyReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.LeakyReLU()\n        )\n\n    \n    def forward(self, x):\n        # Encoder\n        enc0 = self.enc_conv0(x)\n        enc0 = self.res0(enc0)\n        enc1 = self.enc_conv1(self.pool(enc0))\n        enc1 = self.res1(enc1)\n        enc2 = self.enc_conv2(self.pool(enc1))\n        enc2 = self.res2(enc2)\n        enc3 = self.enc_conv3(self.pool(enc2))\n        enc3 = self.res3(enc3)\n        enc3 = torch.flatten(enc3, 1)\n        enc3 = self.lin(enc3)\n        enc3 = enc3.view(-1, 5, 144, 2)\n        \n        return enc3\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.cuda.is_available()\n\n# Initialize model, loss function, and optimizer\nmodel = UNet()\n\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.00000001)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = torch.rand((1, 5, 36,144)).to(device).float()\nmodel(x).shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract one batch from the train loader\ninputs, targets = next(iter(train_loader))\ntargets = torch.real(targets)\n# Select one example (e.g., the first one in the batch)\nexample_fopt = targets[0]  # Shape: [5, 144, 2]\n\n# Select one channel (e.g., the first channel)\nselected_channel = example_fopt[0]  # Shape: [144, 2]\nselected_channel = selected_channel.reshape((12,24))\nprint(selected_channel.shape)\n# Plot the selected channel\nfig, axs = plt.subplots(1, 2, figsize=(6, 4))\naxs[0].imshow(selected_channel, aspect='auto', origin='lower', cmap='turbo')\naxs[0].set_title('Input')\n\n\naxs[1].imshow(selected_channel, aspect='auto', origin='lower', cmap='turbo')\naxs[1].set_title('Predicted')\n\n\n# Plot clean STFT magnitude\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, targets in tqdm(train_loader):\n        inputs = inputs#.to(device).float()\n        targets = targets#.to(device).float()\n        optimizer.zero_grad()\n        outputs = model(inputs*10)\n        #print(outputs)\n        loss = criterion(outputs*10, targets*10)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n\n    # Evaluate the model\n    model.eval()\n    test_loss = 0.0\n    with torch.no_grad():\n        for inputs, targets in tqdm(test_loader):\n            inputs = inputs#.to(device).float()\n            targets = targets#.to(device).float()\n            outputs = model(inputs*10)\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n            \n        if epoch%5 == 0:\n            ii = targets[0][0].reshape((12, 24)).cpu().numpy()\n            jj = outputs[0][0].reshape((12, 24)).cpu().numpy()\n\n            fig, axs = plt.subplots(1, 2, figsize=(6, 4))\n            axs[0].imshow(ii, aspect='auto', origin='lower', cmap='turbo')\n            axs[0].set_title('Target')\n\n\n            axs[1].imshow(jj, aspect='auto', origin='lower', cmap='turbo')\n            axs[1].set_title('Predicted')\n\n            plt.tight_layout()\n            plt.show()\n                \n                \n                \n                \n\n    print(f'Test Loss: {test_loss/len(test_loader)}')","metadata":{},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8619390,"sourceType":"datasetVersion","datasetId":5159300}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-06-06T16:12:25.526362Z","iopub.execute_input":"2024-06-06T16:12:25.526772Z","iopub.status.idle":"2024-06-06T16:12:32.051478Z","shell.execute_reply.started":"2024-06-06T16:12:25.526735Z","shell.execute_reply":"2024-06-06T16:12:32.050026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Main Code Based on CNN*","metadata":{}},{"cell_type":"code","source":"###  Edition 1.1 \n\n\nimport torch\nimport torch.nn as nn\n\nclass ComplexConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n        super(ComplexConv2d, self).__init__()\n        self.real_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        self.imag_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n    \n    def forward(self, x):\n        real = x.real\n        imag = x.imag\n        \n        real_out = self.real_conv(real) - self.imag_conv(imag)\n        imag_out = self.real_conv(imag) + self.imag_conv(real)\n        \n        return torch.complex(real_out, imag_out)\n\n    \nclass ComplexCNN(nn.Module):\n    def __init__(self):\n        super(ComplexCNN, self).__init__()\n        self.conv1 = ComplexConv2d(5, 16, kernel_size=3, padding=1)\n        self.conv2 = ComplexConv2d(16, 32, kernel_size=3, padding=1)\n        self.conv3 = ComplexConv2d(32, 64, kernel_size=3, padding=1)\n        \n        self.fc1 = nn.Linear(64 * 36 * 144, 1024)\n        self.fc2_output1 = nn.Linear(1024, 144 * 2 * 5)\n        self.fc2_output2 = nn.Linear(1024, 36 * 2 * 5)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool2d(x, kernel_size=2)\n        \n        x = self.conv2(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool2d(x, kernel_size=2)\n        \n        x = self.conv3(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool2d(x, kernel_size=2)\n        \n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        \n        x = self.fc1(x)\n        x = nn.functional.relu(x)\n        \n        output1 = self.fc2_output1(x)\n        output1 = output1.view(-1, 144, 2, 5)\n        \n        output2 = self.fc2_output2(x)\n        output2 = output2.view(-1, 36, 2, 5)\n        \n        return output1, output2\n\n\n    \ndef train_model(model, train_loader, criterion, optimizer, num_epochs=25):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, outputs in train_loader:\n            optimizer.zero_grad()\n            outputs1, outputs2 = outputs\n            \n            outputs1_pred, outputs2_pred = model(inputs)\n            \n            loss1 = criterion(outputs1_pred, outputs1)\n            loss2 = criterion(outputs2_pred, outputs2)\n            \n            loss = loss1 + loss2\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * inputs.size(0)\n        \n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}')\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-07T07:00:45.587582Z","iopub.execute_input":"2024-06-07T07:00:45.587983Z","iopub.status.idle":"2024-06-07T07:00:48.515100Z","shell.execute_reply.started":"2024-06-07T07:00:45.587954Z","shell.execute_reply":"2024-06-07T07:00:48.514074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.io import loadmat\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-06-07T07:00:53.010988Z","iopub.execute_input":"2024-06-07T07:00:53.011545Z","iopub.status.idle":"2024-06-07T07:00:53.295402Z","shell.execute_reply.started":"2024-06-07T07:00:53.011509Z","shell.execute_reply":"2024-06-07T07:00:53.294249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(data_folder):\n    input_folder = os.path.join(data_folder, \"Combined Dataset\", \"Input\")\n    output1_folder = os.path.join(data_folder, \"Combined Dataset\", \"Output1\")\n    output2_folder = os.path.join(data_folder, \"Combined Dataset\", \"Output2\")\n    \n    input_files = [os.path.join(input_folder, file) for file in os.listdir(input_folder)]\n    output1_files = [os.path.join(output1_folder, file) for file in os.listdir(output1_folder)]\n    output2_files = [os.path.join(output2_folder, file) for file in os.listdir(output2_folder)]\n    \n    return input_files, output1_files, output2_files","metadata":{"execution":{"iopub.status.busy":"2024-06-07T07:00:59.574805Z","iopub.execute_input":"2024-06-07T07:00:59.575242Z","iopub.status.idle":"2024-06-07T07:00:59.583332Z","shell.execute_reply.started":"2024-06-07T07:00:59.575208Z","shell.execute_reply":"2024-06-07T07:00:59.582089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_mat_file(file_path, key):\n    mat_data = loadmat(file_path)\n    return torch.tensor(mat_data[key], dtype=torch.complex64)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T07:01:05.559804Z","iopub.execute_input":"2024-06-07T07:01:05.560826Z","iopub.status.idle":"2024-06-07T07:01:05.566933Z","shell.execute_reply.started":"2024-06-07T07:01:05.560779Z","shell.execute_reply":"2024-06-07T07:01:05.565421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_data(input_files, output1_files, output2_files):\n    input_real = []\n    input_imag = []\n    output1_real = []\n    output1_imag = []\n    output2_real = []\n    output2_imag = []\n\n    for i in range(len(input_files)):\n        input_data = load_mat_file(input_files[i], 'ch')\n        output1_data = load_mat_file(output1_files[i], 'Fopt_final')\n        output2_data = load_mat_file(output2_files[i], 'Wopt_final')\n\n        input_real.append(input_data.real)\n        input_imag.append(input_data.imag)\n        output1_real.append(output1_data.real)\n        output1_imag.append(output1_data.imag)\n        output2_real.append(output2_data.real)\n        output2_imag.append(output2_data.imag)\n        \n\n    input_real = torch.stack(input_real)\n    input_imag = torch.stack(input_imag)\n    output1_real = torch.stack(output1_real)\n    output1_imag = torch.stack(output1_imag)\n    output2_real = torch.stack(output2_real)\n    output2_imag = torch.stack(output2_imag)\n\n    return input_real, input_imag, output1_real, output1_imag, output2_real, output2_imag","metadata":{"execution":{"iopub.status.busy":"2024-06-07T07:01:08.888832Z","iopub.execute_input":"2024-06-07T07:01:08.889232Z","iopub.status.idle":"2024-06-07T07:01:08.899580Z","shell.execute_reply.started":"2024-06-07T07:01:08.889202Z","shell.execute_reply":"2024-06-07T07:01:08.898353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_complex(real,img):\n    return torch.tensor(real+1j*img,dtype=torch.complex64) \n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T07:01:13.366635Z","iopub.execute_input":"2024-06-07T07:01:13.367332Z","iopub.status.idle":"2024-06-07T07:01:13.372487Z","shell.execute_reply.started":"2024-06-07T07:01:13.367295Z","shell.execute_reply":"2024-06-07T07:01:13.371347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_files,output1_files, output2_files = load_data('/kaggle/input/dataset-final1')","metadata":{"execution":{"iopub.status.busy":"2024-06-07T07:01:17.121556Z","iopub.execute_input":"2024-06-07T07:01:17.121971Z","iopub.status.idle":"2024-06-07T07:01:17.938447Z","shell.execute_reply.started":"2024-06-07T07:01:17.121936Z","shell.execute_reply":"2024-06-07T07:01:17.937354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_real, input_imag, output1_real, output1_imag, output2_real, output2_imag = process_data(input_files,output1_files,output2_files)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T07:02:08.488921Z","iopub.execute_input":"2024-06-07T07:02:08.489362Z","iopub.status.idle":"2024-06-07T07:02:38.909273Z","shell.execute_reply.started":"2024-06-07T07:02:08.489330Z","shell.execute_reply":"2024-06-07T07:02:38.908064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(input_real.shape)\nprint(output1_real.shape)\nprint(output2_real.shape)\nprint(input_imag.shape)\nprint(output1_imag.shape)\nprint(output2_imag.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T07:05:05.492510Z","iopub.execute_input":"2024-06-07T07:05:05.493357Z","iopub.status.idle":"2024-06-07T07:05:05.501236Z","shell.execute_reply.started":"2024-06-07T07:05:05.493316Z","shell.execute_reply":"2024-06-07T07:05:05.499733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = convert_to_complex(input_real,input_imag)\noutputs1 = convert_to_complex(output1_real,output1_imag)\noutputs2 = convert_to_complex(output2_real,output2_imag)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T07:05:17.885523Z","iopub.execute_input":"2024-06-07T07:05:17.885935Z","iopub.status.idle":"2024-06-07T07:05:19.373376Z","shell.execute_reply.started":"2024-06-07T07:05:17.885905Z","shell.execute_reply":"2024-06-07T07:05:19.372343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\n\n# Assuming data is loaded in variables inputs, outputs1, and outputs2\ndataset = TensorDataset(inputs, outputs1, outputs2)\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\nmodel = ComplexCNN()\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\nmodel = train_model(model, train_loader, criterion, optimizer, num_epochs=25)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T19:11:57.829727Z","iopub.execute_input":"2024-06-06T19:11:57.830209Z","iopub.status.idle":"2024-06-06T19:12:03.773010Z","shell.execute_reply.started":"2024-06-06T19:11:57.830175Z","shell.execute_reply":"2024-06-06T19:12:03.771547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define the complex tensors\ninputs_complex = torch.tensor(input_real, dtype=torch.float32) + 1j * torch.tensor(input_imag, dtype=torch.float32)\noutputs1_complex = torch.tensor(output1_real, dtype=torch.float32) + 1j * torch.tensor(output1_imag, dtype=torch.float32)\n\ninputs = torch.tensor(inputs_complex, dtype=torch.complex64)\noutputs1 = torch.tensor(outputs1_complex, dtype=torch.complex64)\n\n# Assuming outputs2 is defined similarly\noutputs2_complex = torch.tensor(output2_real, dtype=torch.float32) + 1j * torch.tensor(output2_imag, dtype=torch.float32)\noutputs2 = torch.tensor(outputs2_complex, dtype=torch.complex64)\n\n# Create the dataset and dataloader\ndataset = TensorDataset(inputs, outputs1, outputs2)\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Define the model (assuming ComplexCNN is defined)\nmodel = ComplexCNN()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ndef train_model(model, train_loader, criterion, optimizer, num_epochs=25):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, outputs1, outputs2 in train_loader:\n            optimizer.zero_grad()\n            # Assuming the model returns two outputs\n            outputs_pred1, outputs_pred2 = model(inputs)\n            loss1 = criterion(outputs_pred1, outputs1)\n            loss2 = criterion(outputs_pred2, outputs2)\n            loss = loss1 + loss2\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n\n    return model\n\n# Train the model\nmodel = train_model(model, train_loader, criterion, optimizer, num_epochs=25)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Edition 1.2\n*Error in codes*","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport scipy.io\n\n# Define a custom dataset to handle the complex data\nclass ComplexDataset(Dataset):\n    def __init__(self, inputs, outputs1, outputs2):\n        self.inputs = inputs\n        self.outputs1 = outputs1\n        self.outputs2 = outputs2\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        return self.inputs[idx], (self.outputs1[idx], self.outputs2[idx])\n'''\n# Load .mat files\ninputs_mat = scipy.io.loadmat('inputs.mat')['inputs']\noutputs1_mat = scipy.io.loadmat('outputs1.mat')['outputs1']\noutputs2_mat = scipy.io.loadmat('outputs2.mat')['outputs2']\n'''\n# Convert to PyTorch tensors\ninputs = torch.tensor(inputs_mat, dtype=torch.complex64)\noutputs1 = torch.tensor(outputs1_mat, dtype=torch.complex64)\noutputs2 = torch.tensor(outputs2_mat, dtype=torch.complex64)\n\n\n# Create dataset and dataloader\ndataset = ComplexDataset(inputs, outputs1, outputs2)\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Define the complex convolutional layer\nclass ComplexConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n        super(ComplexConv2d, self).__init__()\n        self.real_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        self.imag_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n    \n    def forward(self, x):\n        real = x.real\n        imag = x.imag\n        \n        real_out = self.real_conv(real) - self.imag_conv(imag)\n        imag_out = self.real_conv(imag) + self.imag_conv(real)\n        \n        return torch.complex(real_out, imag_out)\n\n# Define the complex CNN model\nclass ComplexCNN(nn.Module):\n    def __init__(self):\n        super(ComplexCNN, self).__init__()\n        self.conv1 = ComplexConv2d(5, 16, kernel_size=3, padding=1)\n        self.conv2 = ComplexConv2d(16, 32, kernel_size=3, padding=1)\n        self.conv3 = ComplexConv2d(32, 64, kernel_size=3, padding=1)\n        \n        self.fc1 = nn.Linear(64 * 36 * 144, 1024)\n        self.fc2_output1 = nn.Linear(1024, 144 * 2 * 5)\n        self.fc2_output2 = nn.Linear(1024, 36 * 2 * 5)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool2d(x, kernel_size=2)\n        \n        x = self.conv2(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool2d(x, kernel_size=2)\n        \n        x = self.conv3(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool2d(x, kernel_size=2)\n        \n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        \n        x = self.fc1(x)\n        x = nn.functional.relu(x)\n        \n        output1 = self.fc2_output1(x)\n        output1 = output1.view(-1, 144, 2, 5)\n        \n        output2 = self.fc2_output2(x)\n        output2 = output2.view(-1, 36, 2, 5)\n        \n        return output1, output2\n\n# Define the training loop\ndef train_model(model, train_loader, criterion, optimizer, num_epochs=25):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, outputs in train_loader:\n            optimizer.zero_grad()\n            outputs1, outputs2 = outputs\n            \n            outputs1_pred, outputs2_pred = model(inputs)\n            \n            loss1 = criterion(outputs1_pred, outputs1)\n            loss2 = criterion(outputs2_pred, outputs2)\n            \n            loss = loss1 + loss2\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * inputs.size(0)\n        \n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}')\n    return model\n\n# Initialize the model, criterion, and optimizer\nmodel = ComplexCNN()\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\nmodel = train_model(model, train_loader, criterion, optimizer, num_epochs=25)\n","metadata":{"_kg_hide-input":false,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}